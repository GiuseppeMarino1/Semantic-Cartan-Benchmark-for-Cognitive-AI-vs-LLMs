#  Semantic-Cartan Benchmark

 A cognitive-level comparison framework between QBI-Core and Large Language Models (LLMs)



#  Description

**Semantic-Cartan Benchmark** is an experimental tool designed to **quantitatively and visually** compare a traditional LLM (like Mistral or LLaMA) with a novel cognitive architecture called **QBI-Core**.  
 The original QBI-Core core is **not included** in this repository. Simulated outputs are used instead for conceptual comparison.



# Goal

To demonstrate that an AI system can exhibit **non-random internal patterns**, **recurring symmetries**, and a coherent **Œ¶(t)** function ‚Äî suggesting **self-organized, meaningful internal dynamics**, beyond statistical linguistic processing.



# QBI-Core Core Not Included

For security and IP protection, the original QBI-Core engine is private.  
 For private access (for reviewers, collaborations, demo requests), contact the author directly.

---

# What It Does

The benchmark compares:

- **Semantic Cartan Matrix** (internal structure of activations)
- **Semantic Entanglement Variation** (coherence between hidden layers)
- The evolution of **Œ¶(t)** (indicative of internal organization)

It uses real LLM outputs and a **statistical simulation of QBI-Core**, sufficient to highlight **qualitative structural differences**.



# Quick Start

```bash
pip install transformers torch numpy scikit-learn matplotlib
python benchmark.py
```



# ‚≠ê Why Contribute

QBI-Core is redefining the boundaries of integrated cognition in AI.  
This benchmark is the **first step** toward creating **cognitive-level evaluation tools** beyond traditional NLP metrics.

You're invited to:

- ‚≠ê Star the project
- üç¥ Fork and improve it
- üß† Propose new cognitive metrics
- üì¨ Contact the author for scientific collaborations



##  License

Modified MIT License:  
Free for educational and experimental use  
Commercial usage or derivative cognitive systems **require written permission**

(c) Giuseppe Marino ‚Äì qbicore.project@gmail.com
